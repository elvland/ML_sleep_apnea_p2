{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8etKxuPeUACP"
      },
      "source": [
        "---\n",
        "---\n",
        "# [PyTorch](https://pytorch.org/) </br>\n",
        "\n",
        "<img src= data:image/jpeg;base64,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>\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4wVsezFdevG"
      },
      "source": [
        "# **PyTorch** Intro and <font color= magentea>Autograd</font> <font color=#ff80ff>(**Automatic Differentiation**</font>)\n",
        "---\n",
        "\n",
        "---\n",
        "---\n",
        "## <font size=5>MACHINE LEARNING LAB (MLLB)</font>\n",
        "### ETSIT-UPM MUIT & MUTSC\n",
        "\n",
        "---\n",
        "---\n",
        "\n",
        "## A condensation of:\n",
        "* PyTorch Tutorial 02 - Tensor Basics\n",
        "https://www.youtube.com/watch?v=exaWOE8jvy8\n",
        "* PyTorch Tutorial 03 - Gradient Calculation With Autograd https://www.youtube.com/watch?v=DbeIqrwb_dE\n",
        "* PyTorch Tutorial 04 - Backpropagation - Theory With Example https://www.youtube.com/watch?v=3Kb0QS6z7WA&list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4&index=4\n",
        "* PyTorch Tutorial 05 - Gradient Descent with Autograd and Backpropagation https://www.youtube.com/watch?v=E-I2DNVzQLg\n",
        "* PyTorch Tutorial 06 - Training Pipeline: Model, Loss, and Optimizer https://www.youtube.com/watch?v=VVDHU_TWwUg&list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4&index=6\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6dcF1cdcL71"
      },
      "source": [
        "import torch"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ziHfKEKca7f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf1599ea-8482-4a64-bbe1-a438858a676c"
      },
      "source": [
        "print(torch.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.12.1+cu113\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2QsxxTCVjLm"
      },
      "source": [
        "## **PyTorch Tutorial 02 - Tensor Basics** https://www.youtube.com/watch?v=exaWOE8jvy8"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPMp4AxaWLct",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eacb0f95-7e89-49cc-cee1-1bcff305da6a"
      },
      "source": [
        "x = torch.tensor([1,2,3])\n",
        "print(x)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1, 2, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGWIGEI7mGPA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1665f1f9-d7f6-452d-f015-f483579a7f7f"
      },
      "source": [
        "x = torch.rand(4,2,3)\n",
        "print(x)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0.9800, 0.6618, 0.0185],\n",
            "         [0.2479, 0.5197, 0.3052]],\n",
            "\n",
            "        [[0.8263, 0.0739, 0.9918],\n",
            "         [0.1657, 0.6569, 0.8266]],\n",
            "\n",
            "        [[0.5536, 0.6012, 0.6259],\n",
            "         [0.7833, 0.3580, 0.4834]],\n",
            "\n",
            "        [[0.4868, 0.6802, 0.5830],\n",
            "         [0.5643, 0.4304, 0.6004]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1mnqv4WXdS-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35980426-7b91-4352-bcca-48069fee6d7f"
      },
      "source": [
        "x.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 2, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cl03jO30Yo4D"
      },
      "source": [
        "* # Combination with Numpy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_uTVsT9WAse",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90eb6a1c-a4f4-4a4c-8fad-63c0e1638a6a"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "a = np.ones(5)\n",
        "print(a)\n",
        "\n",
        "b = torch.from_numpy(a)\n",
        "print(b)\n",
        "\n",
        "a += 1\n",
        "print(a)\n",
        "print(b)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1. 1. 1. 1. 1.]\n",
            "tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n",
            "[2. 2. 2. 2. 2.]\n",
            "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CoEB8eTcvB0"
      },
      "source": [
        "---\n",
        "---\n",
        "\n",
        "# **PyTorch Tutorial 04** - Backpropagation - Theory With Example\n",
        "\n",
        "https://www.youtube.com/watch?v=3Kb0QS6z7WA&list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4&index=4\n",
        "\n",
        "# **PyTorch Tutorial 05** - Gradient Descent with Autograd and Backpropagation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3HME0A1Ycw9"
      },
      "source": [
        "---\n",
        "---\n",
        "\n",
        "# Operations create a **COMPUTATIONAL GRAPH**\n",
        "\n",
        "- ## Example: a simple linear regression\n",
        "\n",
        "<img src = 'https://drive.google.com/uc?id=1Isslg5y3HmXxUROCPLSL_8EVyRcNcMcb' />\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afSgDW3-b-Z2"
      },
      "source": [
        "* # In the example:\n",
        "> * ## $x$ and $y$ are inputs\n",
        "> * ## $w$ is the weigth to be trained: using backpropagation and Stochastic Gradient Descent (SGD)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxSi0PGKgOBU"
      },
      "source": [
        "## PyTorch Tutorial 05 - Gradient Descent with Autograd and Backpropagation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "GpDnXJV6yxCR",
        "outputId": "3bd869a9-4e0f-4c9e-a2f4-c8433ec35c96"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "# Linear Regression\n",
        "# f = 2 * x\n",
        "# CREATE SOME Training data\n",
        "X = np.array([1, 2, 3, 4], dtype = np.float32)\n",
        "Y = np.array([2, 4, 6, 8], dtype = np.float32)\n",
        "\n",
        "plt.scatter(X,Y)\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('Y')\n",
        "plt.title('Simple Regression (Training Data)')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Simple Regression (Training Data)')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVG5OGMsiGND"
      },
      "source": [
        "- ### PyTorch Autograd provides the gradients \"automatically\"\n",
        "\n",
        "\n",
        "1.   Note that $w$ tensor **requires_grad=True**\n",
        "\n",
        "2.   **update weights** are NOT in the Computational Graph\n",
        "\n",
        "3.   In every epoch **w.grad.zero_()** to avoid gradients accumulation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1t7gM1dRez_J"
      },
      "source": [
        "# Using PyTorch\n",
        "X = torch.tensor([1, 2, 3, 4], dtype = torch.float32)\n",
        "Y = torch.tensor([2, 4, 6, 8], dtype = torch.float32)\n",
        "w = torch.tensor(0.0, dtype = torch.float32, requires_grad=True)\n",
        "\n",
        "# model prediction\n",
        "def forward(x):\n",
        "  return w * x\n",
        "\n",
        "# loss = MSE\n",
        "def loss(y, y_predicted):\n",
        "  return ((y_predicted - y)**2).mean()\n",
        "\n",
        "# gradient: specific calculation is NOT NEEDED as PyTorch provides it !!!\n",
        "# MSE = 1/N * (w*x - y)**2\n",
        "# dJ/dw = 1/N 2x (w*x-y)\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oc4eXHOifNbF",
        "outputId": "8157adcd-0aab-4bbc-bd23-63b3d8a43b50"
      },
      "source": [
        " y_pred = forward(X)\n",
        " print('Foward predicted values: ', y_pred)\n",
        " print('Initial w: ', w)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Foward predicted values:  tensor([0., 0., 0., 0.], grad_fn=<MulBackward0>)\n",
            "Initial w:  tensor(0., requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jts4TreCfNbG",
        "outputId": "920e0033-06c0-4d32-9348-3af4e16cc671"
      },
      "source": [
        " l = loss(Y, y_pred)\n",
        "\n",
        " print('Loss: ', l)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss:  tensor(30., grad_fn=<MeanBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3x-lVaAfNbH",
        "outputId": "f6e17650-f411-4234-f052-3c70d7b04076"
      },
      "source": [
        "# INSTEAD OF calculating gradients: w_grad = gradient(X,Y,y_pred)\n",
        "\n",
        "# PyTorch provide them:\n",
        "# Autograd provides the gradients \"automatically\"\n",
        "\n",
        "l.backward() # dl/dw\n",
        "\n",
        "with torch.no_grad():   # THIS is because we don't want this to be part of the computational graph\n",
        "  print('Gradient: ', w.grad)\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient:  tensor(-30.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQLycTbOgI8N",
        "outputId": "59078c8a-1229-4e00-8045-ff86b5ef6615"
      },
      "source": [
        "learning_rate = 0.01\n",
        "\n",
        "print('Initial weight: ', w)\n",
        "\n",
        "# update weights\n",
        "with torch.no_grad():   # THIS is because we don't want this to be part of the computational graph\n",
        "    w -= learning_rate * w.grad\n",
        "\n",
        "\n",
        "print('Updated weight: ', w)\n",
        "\n",
        "# zero gradients: to avoid gradients accumulation!!!\n",
        "w.grad.zero_()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial weight:  tensor(0., requires_grad=True)\n",
            "Updated weight:  tensor(0.3000, requires_grad=True)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEchbTrOiYh6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29799f97-6b3e-4260-dbfc-e4807e4e8ea4"
      },
      "source": [
        "# Training using PyTorch\n",
        "w = torch.tensor(0.0, dtype = torch.float32, requires_grad=True)\n",
        "\n",
        "learning_rate = 0.01\n",
        "n_iters =20\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "  # prediction (forward)\n",
        "  y_pred = forward(X)\n",
        "\n",
        "  # loss\n",
        "  l = loss(Y, y_pred)\n",
        "\n",
        "  # gradients\n",
        "  #dw = gradient(X,Y,y_pred)\n",
        "  l.backward() # dl/dw\n",
        "\n",
        "  # update weights\n",
        "  with torch.no_grad():   # THIS is because we don't want this to be part of the computational graph\n",
        "    w -= learning_rate * w.grad\n",
        "\n",
        "  # zero gradients\n",
        "  w.grad.zero_()\n",
        "\n",
        "\n",
        "  if epoch % 1 == 0:\n",
        "    print(f'epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}')\n",
        "\n",
        "print(f'Prediction after training fro x = 5: f(5) = {forward(5):.3f}')\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1: w = 0.300, loss = 30.00000000\n",
            "epoch 2: w = 0.555, loss = 21.67499924\n",
            "epoch 3: w = 0.772, loss = 15.66018772\n",
            "epoch 4: w = 0.956, loss = 11.31448650\n",
            "epoch 5: w = 1.113, loss = 8.17471695\n",
            "epoch 6: w = 1.246, loss = 5.90623236\n",
            "epoch 7: w = 1.359, loss = 4.26725292\n",
            "epoch 8: w = 1.455, loss = 3.08308983\n",
            "epoch 9: w = 1.537, loss = 2.22753215\n",
            "epoch 10: w = 1.606, loss = 1.60939169\n",
            "epoch 11: w = 1.665, loss = 1.16278565\n",
            "epoch 12: w = 1.716, loss = 0.84011245\n",
            "epoch 13: w = 1.758, loss = 0.60698116\n",
            "epoch 14: w = 1.794, loss = 0.43854395\n",
            "epoch 15: w = 1.825, loss = 0.31684780\n",
            "epoch 16: w = 1.851, loss = 0.22892261\n",
            "epoch 17: w = 1.874, loss = 0.16539653\n",
            "epoch 18: w = 1.893, loss = 0.11949898\n",
            "epoch 19: w = 1.909, loss = 0.08633806\n",
            "epoch 20: w = 1.922, loss = 0.06237914\n",
            "Prediction after training fro x = 5: f(5) = 9.612\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-UwbTcvjnOe"
      },
      "source": [
        "## PyTorch Tutorial 06 - Training Pipeline: Model, Loss, and Optimizer\n",
        "\n",
        "https://www.youtube.com/watch?v=VVDHU_TWwUg&list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4&index=6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7a2pNw5_GjH"
      },
      "source": [
        "## Training PIPELINE\n",
        "\n",
        "* 1) Design model (input, output size, forward pass)\n",
        "* 2) Construct loss and optimizer\n",
        "* 3) Training loop:\n",
        "\n",
        "  - forward pass: compute prediction\n",
        "  - backward pass: gradients\n",
        "  - update weights\n",
        "\n",
        "# ... and using torch.nn to define the model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fLXfBn435nz"
      },
      "source": [
        "* # PyTorch Training PIPELINE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EsU96pmL34Us",
        "outputId": "18c6ea91-6db1-4707-938e-93f8f09ae7cd"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "# We must separate each individual training data\n",
        "X = torch.tensor([[1],[2], [3], [4]], dtype = torch.float32)\n",
        "Y = torch.tensor([[2], [4], [6], [8]], dtype = torch.float32)\n",
        "\n",
        "X_test = torch.tensor([[5]], dtype = torch.float32)\n",
        "\n",
        "n_input_samples, n_input_features = X.shape\n",
        "n_output_samples, n_output_features = Y.shape\n",
        "\n",
        "input_size =  n_input_features\n",
        "output_size = n_output_features\n",
        "\n",
        "print('input_size:', input_size, 'output_size:' ,\\\n",
        "      output_size, 'n_input_samples: ', n_input_samples, \\\n",
        "      'n_output_samples: ', n_output_samples)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_size: 1 output_size: 1 n_input_samples:  4 n_output_samples:  4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ### The <font color=yellow>torch.nn</font> namespace provides all the building blocks you need to build your own neural network. Every module in PyTorch subclasses the nn.Module. \n",
        "\n",
        "> #### nn.Linear() Applies a linear transformation to the incoming data: y=x*w+b"
      ],
      "metadata": {
        "id": "p5FwNR6sE9FZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a linear regression model\n",
        "model = nn.Linear(input_size, output_size)\n",
        "\n",
        "# loss = MSE\n",
        "loss = nn.MSELoss()\n",
        "\n",
        "print(f'Prediction before training for X_test (only ONE value):  {model(X_test).item():.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQ0dniq5FWMR",
        "outputId": "adee85e8-5d91-4a63-ffa7-2933588af256"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction before training for X_test (only ONE value):  4.379\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byhH3gGM5gVf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25467ba9-a02c-4308-8702-cd63dd1def3b"
      },
      "source": [
        "# PyTorch Training PIPELINE\n",
        "\n",
        "learning_rate = 0.01\n",
        "n_iters =200\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "  # prediction (forward)\n",
        "  y_pred = model(X)\n",
        "\n",
        "  # loss\n",
        "  l = loss(Y, y_pred)\n",
        "\n",
        "  # gradients\n",
        "  l.backward()\n",
        "\n",
        "  # Optimizer to update weights\n",
        "  optimizer.step()\n",
        "\n",
        "  # zero gradients (avoid gradients accumulation)\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "\n",
        "  if epoch % 10 == 0:\n",
        "    [w,b] = model.parameters()\n",
        "    print(f'epoch {epoch+1}: w = {w[0][0].item():.3f}, b = {b.item():.3f}, loss = {l:.8f}')\n",
        "\n",
        "print(f'\\nFor X_test = {X_test.item():.3f}, the prediction is =  {model(X_test).item():.3f}')\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1: w = 0.858, b = 0.895, loss = 7.78769159\n",
            "epoch 11: w = 1.500, b = 1.075, loss = 0.40510470\n",
            "epoch 21: w = 1.612, b = 1.077, loss = 0.20224786\n",
            "epoch 31: w = 1.639, b = 1.050, loss = 0.18583770\n",
            "epoch 41: w = 1.652, b = 1.020, loss = 0.17490098\n",
            "epoch 51: w = 1.663, b = 0.990, loss = 0.16471754\n",
            "epoch 61: w = 1.673, b = 0.961, loss = 0.15513012\n",
            "epoch 71: w = 1.683, b = 0.933, loss = 0.14610073\n",
            "epoch 81: w = 1.692, b = 0.905, loss = 0.13759692\n",
            "epoch 91: w = 1.701, b = 0.878, loss = 0.12958813\n",
            "epoch 101: w = 1.710, b = 0.852, loss = 0.12204538\n",
            "epoch 111: w = 1.719, b = 0.827, loss = 0.11494168\n",
            "epoch 121: w = 1.727, b = 0.803, loss = 0.10825150\n",
            "epoch 131: w = 1.735, b = 0.779, loss = 0.10195073\n",
            "epoch 141: w = 1.743, b = 0.756, loss = 0.09601671\n",
            "epoch 151: w = 1.750, b = 0.734, loss = 0.09042801\n",
            "epoch 161: w = 1.758, b = 0.712, loss = 0.08516461\n",
            "epoch 171: w = 1.765, b = 0.691, loss = 0.08020768\n",
            "epoch 181: w = 1.772, b = 0.671, loss = 0.07553908\n",
            "epoch 191: w = 1.779, b = 0.651, loss = 0.07114238\n",
            "\n",
            "For X_test = 5.000, the prediction is =  9.556\n"
          ]
        }
      ]
    }
  ]
}